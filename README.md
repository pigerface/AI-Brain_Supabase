# Supabase æœ¬åœ°è‡ªè¨—ç®¡ RAG ç³»çµ±

ä¸€å€‹åŠŸèƒ½å®Œæ•´çš„ Supabase æœ¬åœ°éƒ¨ç½²è§£æ±ºæ–¹æ¡ˆï¼Œå°ˆç‚º StatementDog RAG (Retrieval-Augmented Generation) ç³»çµ±è¨­è¨ˆï¼Œæ•´åˆäº† PostgreSQL + pgvector å‘é‡è³‡æ–™åº«ã€å…¨æ–‡æœç´¢ã€å³æ™‚é€šè¨Šå’Œæª”æ¡ˆå­˜å„²ã€‚

[![Python](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://python.org)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15%2B-green.svg)](https://postgresql.org)
[![Docker](https://img.shields.io/badge/Docker-Required-blue.svg)](https://docker.com)
[![UV](https://img.shields.io/badge/UV-Package%20Manager-purple.svg)](https://github.com/astral-sh/uv)

## ğŸŒŸ æ ¸å¿ƒç‰¹è‰²

### ğŸ  å®Œå…¨æœ¬åœ°åŒ–éƒ¨ç½²
- **é›¶é›²ç«¯ä¾è³´**ï¼šå®Œæ•´çš„è‡ªè¨—ç®¡æ–¹æ¡ˆï¼Œç„¡éœ€å¤–éƒ¨æœå‹™
- **ç”Ÿç”¢å°±ç·’**ï¼šåŒ…å«èªè­‰ã€æˆæ¬Šã€å­˜å„²ã€å³æ™‚åŠŸèƒ½
- **å®¹å™¨åŒ–éƒ¨ç½²**ï¼šä½¿ç”¨ Docker Compose ä¸€éµå•Ÿå‹•æ‰€æœ‰æœå‹™
- **æ•¸æ“šä¸»æ¬Š**ï¼šå®Œå…¨æ§åˆ¶æ‚¨çš„æ•¸æ“šå’Œéš±ç§

### ğŸ§  æ™ºèƒ½ RAG æ¶æ§‹
- **å¤šç¨®æœç´¢æ¨¡å¼**ï¼šå‘é‡æœç´¢ã€å…¨æ–‡æœç´¢ã€æ··åˆæœç´¢
- **pgvector æ•´åˆ**ï¼šé«˜æ•ˆèƒ½å‘é‡ç›¸ä¼¼åº¦è¨ˆç®—
- **å½ˆæ€§åµŒå…¥æ¨¡å‹**ï¼šæ”¯æ´å¤šç¨® AI æ¨¡å‹çš„åµŒå…¥å‘é‡
- **æ™ºèƒ½åˆ†å¡Š**ï¼šæ–‡æª”æ™ºèƒ½åˆ‡åˆ†å’Œç´¢å¼•

### ğŸ›  é–‹ç™¼è€…å‹å¥½
- **çµ±ä¸€å®¢æˆ¶ç«¯**ï¼šæ•´åˆ SQLAlchemy ORM å’Œ Supabase SDK
- **UV Workspace**ï¼šç¾ä»£åŒ– Python ä¾è³´ç®¡ç†
- **CLI å·¥å…·**ï¼šå‘½ä»¤åˆ—ç®¡ç†å’Œæ“ä½œä»‹é¢
- **å‹åˆ¥å®‰å…¨**ï¼šå®Œæ•´çš„ TypeScript/Python å‹åˆ¥å®šç¾©

### ğŸ“Š ä¼æ¥­ç´šåŠŸèƒ½
- **Row Level Security**ï¼šç²¾ç´°çš„æ¬Šé™æ§åˆ¶
- **å³æ™‚é€šè¨Š**ï¼šWebSocket æ”¯æ´å¯¦æ™‚æ•¸æ“šåŒæ­¥
- **æª”æ¡ˆå­˜å„²**ï¼šS3 ç›¸å®¹çš„å°è±¡å­˜å„²
- **ç›£æ§å„€è¡¨æ¿**ï¼šSupabase Studio ç®¡ç†ä»‹é¢

## ğŸ— ç³»çµ±æ¶æ§‹

```mermaid
graph TB
    subgraph "Client Layer"
        CLI[CLI Tools]
        SDK[Python SDK]
        APP[Applications]
    end
    
    subgraph "API Gateway"
        KONG[Kong Gateway]
        REST[REST API]
        GRAPHQL[GraphQL]
        REALTIME[Realtime]
    end
    
    subgraph "Core Services"
        AUTH[GoTrue Auth]
        STORAGE[Storage API]
        FUNCTIONS[Edge Functions]
    end
    
    subgraph "Database Layer"
        PG[(PostgreSQL 15)]
        VECTOR[pgvector]
        FTS[Full-Text Search]
    end
    
    subgraph "Management"
        STUDIO[Supabase Studio]
        LOGS[Logflare Analytics]
    end
    
    CLI --> KONG
    SDK --> KONG
    APP --> KONG
    
    KONG --> REST
    KONG --> GRAPHQL
    KONG --> REALTIME
    
    REST --> PG
    GRAPHQL --> PG
    AUTH --> PG
    STORAGE --> PG
    
    PG --> VECTOR
    PG --> FTS
    
    STUDIO --> PG
    LOGS --> PG
```

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
supabase/
â”œâ”€â”€ ğŸ“‹ README.md                    # æœ¬æ–‡æª”
â”œâ”€â”€ ğŸ³ docker-compose.yml           # å®¹å™¨ç·¨æ’é…ç½®
â”œâ”€â”€ ğŸ“„ pyproject.toml               # Python å°ˆæ¡ˆé…ç½® (UV)
â”œâ”€â”€ ğŸ”§ supabase.sh                  # ç®¡ç†è…³æœ¬
â”œâ”€â”€ ğŸ“ .env.example                 # ç’°å¢ƒè®Šæ•¸ç¯„æœ¬
â”œâ”€â”€ ğŸ¯ uv-example.py                # Python SDK ä½¿ç”¨ç¯„ä¾‹
â”‚
â”œâ”€â”€ ğŸ“‚ client/                      # Python å®¢æˆ¶ç«¯å¥—ä»¶
â”‚   â”œâ”€â”€ __init__.py                 # å¥—ä»¶åˆå§‹åŒ–
â”‚   â”œâ”€â”€ ğŸ”— client.py                # çµ±ä¸€å®¢æˆ¶ç«¯é¡åˆ¥
â”‚   â”œâ”€â”€ âš™ï¸  config.py               # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ ğŸ—„ database.py              # SQLAlchemy ORM æ¨¡å‹
â”‚   â””â”€â”€ ğŸ’» cli.py                   # å‘½ä»¤åˆ—å·¥å…·
â”‚
â”œâ”€â”€ ğŸ“‚ sql/                         # è³‡æ–™åº« Schema
â”‚   â””â”€â”€ init_schema.sql             # RAG ç³»çµ±åˆå§‹ Schema
â”‚
â”œâ”€â”€ ğŸ“‚ volumes/                     # æŒä¹…åŒ–æ•¸æ“š (Docker æ›è¼‰)
â”‚   â”œâ”€â”€ db/                         # PostgreSQL æ•¸æ“šç›®éŒ„
â”‚   â”œâ”€â”€ storage/                    # æª”æ¡ˆå­˜å„²ç›®éŒ„
â”‚   â”œâ”€â”€ logs/                       # æ—¥èªŒæª”æ¡ˆ
â”‚   â””â”€â”€ api/                        # API Gateway é…ç½®
â”‚
â””â”€â”€ ğŸ“‚ tests/                       # å–®å…ƒæ¸¬è©¦ (å¯é¸)
    â”œâ”€â”€ test_client.py
    â”œâ”€â”€ test_database.py
    â””â”€â”€ fixtures/
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### å‰ç½®æ¢ä»¶

ç¢ºä¿æ‚¨çš„ç³»çµ±å·²å®‰è£ä»¥ä¸‹å·¥å…·ï¼š

- **Docker** 20.10+ ([å®‰è£æŒ‡å—](https://docs.docker.com/get-docker/))
- **Docker Compose** 2.0+ ([å®‰è£æŒ‡å—](https://docs.docker.com/compose/install/))
- **UV** 0.1.0+ ([å®‰è£æŒ‡å—](https://github.com/astral-sh/uv#installation))

```bash
# å¿«é€Ÿå®‰è£ UV (æ¨è–¦)
curl -LsSf https://astral.sh/uv/install.sh | sh

# é©—è­‰å®‰è£
docker --version
docker compose version
uv --version
```

### 1. ğŸ”„ åˆå§‹è¨­ç½®

```bash
# 1. å…‹éš†å°ˆæ¡ˆ (å¦‚æœå°šæœªå®Œæˆ)
git clone <your-repo> statementdog
cd statementdog/supabase

# 2. é¦–æ¬¡è¨­ç½® - è‡ªå‹•å‰µå»ºç›®éŒ„å’Œé…ç½®æª”æ¡ˆ
./supabase.sh setup

# 3. é…ç½®ç’°å¢ƒè®Šæ•¸ (é‡è¦ï¼)
cp .env.example .env
nano .env  # æˆ–ä½¿ç”¨æ‚¨åå¥½çš„ç·¨è¼¯å™¨

# 4. âš ï¸ å¿…é ˆä¿®æ”¹çš„å®‰å…¨è¨­å®š
# - POSTGRES_PASSWORD: è¨­ç½®å¼·å¯†ç¢¼
# - JWT_SECRET: 32å­—ç¬¦éš¨æ©Ÿå­—ç¬¦ä¸²
# - ANON_KEY & SERVICE_ROLE_KEY: ä½¿ç”¨ JWT_SECRET ç”Ÿæˆ
```

### 2. ğŸ” å®‰å…¨é…ç½®

ç·¨è¼¯ `.env` æª”æ¡ˆä¸¦ä¿®æ”¹ä»¥ä¸‹é—œéµè¨­å®šï¼š

```bash
# ğŸ”‘ è³‡æ–™åº«å®‰å…¨ (å¿…é ˆä¿®æ”¹)
POSTGRES_PASSWORD=your_super_secure_password_here

# ğŸ”‘ JWT è¨­å®š (å¿…é ˆä¿®æ”¹)
JWT_SECRET=your_32_character_random_secret_key

# ğŸ”‘ API é‡‘é‘° (ä½¿ç”¨ JWT_SECRET ç”Ÿæˆ)
ANON_KEY=your_generated_anon_key
SERVICE_ROLE_KEY=your_generated_service_role_key

# ğŸ“§ éƒµä»¶è¨­å®š (å¯é¸ï¼Œç”¨æ–¼èªè­‰)
SMTP_HOST=smtp.gmail.com
SMTP_USER=your-email@gmail.com  
SMTP_PASS=your-app-password

# ğŸŒ å…¬é–‹ URL (ç”¨æ–¼å›èª¿)
SUPABASE_PUBLIC_URL=http://localhost:8000
```

> ğŸ’¡ **API é‡‘é‘°ç”Ÿæˆ**ï¼šä½¿ç”¨ [Supabase JWT ç”¢ç”Ÿå™¨](https://supabase.com/docs/guides/self-hosting/docker#generate-api-keys) æˆ–åƒè€ƒ Supabase å®˜æ–¹æ–‡æª”ã€‚

### 3. ğŸŒŸ å•Ÿå‹•æœå‹™

```bash
# å•Ÿå‹•æ‰€æœ‰ Supabase æœå‹™
./supabase.sh start

# æŸ¥çœ‹æœå‹™ç‹€æ…‹
./supabase.sh status

# æŸ¥çœ‹æ—¥èªŒ (å¯é¸)
./supabase.sh logs
```

### 4. âœ… é©—è­‰éƒ¨ç½²

```bash
# å¥åº·æª¢æŸ¥
curl http://localhost:8000/health

# ä½¿ç”¨ Python å®¢æˆ¶ç«¯æ¸¬è©¦
uv run python -c "
from client import create_client
client = create_client()
print('âœ… é€£æ¥æˆåŠŸ:', client.health_check())
"
```

### 5. ğŸ¯ è¨ªå•æœå‹™

å•Ÿå‹•æˆåŠŸå¾Œï¼Œæ‚¨å¯ä»¥è¨ªå•ï¼š

| æœå‹™ | URL | æè¿° |
|------|-----|------|
| ğŸ› **Supabase Studio** | http://localhost:8000 | ç®¡ç†ä»‹é¢å’Œè³‡æ–™åº«ç€è¦½å™¨ |
| ğŸš€ **REST API** | http://localhost:8000/rest/v1 | RESTful API ç«¯é» |
| ğŸ“¡ **GraphQL** | http://localhost:8000/graphql/v1 | GraphQL æŸ¥è©¢ä»‹é¢ |
| âš¡ **Realtime** | ws://localhost:8000/realtime/v1 | WebSocket å³æ™‚é€šè¨Š |
| ğŸ—ƒ **PostgreSQL** | localhost:5432 | ç›´æ¥è³‡æ–™åº«é€£æ¥ |

## ğŸ“Š RAG è³‡æ–™åº«æ¶æ§‹

### æ ¸å¿ƒè³‡æ–™è¡¨

æˆ‘å€‘çš„ RAG ç³»çµ±æ¡ç”¨äº†å„ªåŒ–çš„è³‡æ–™åº«çµæ§‹ï¼Œæ”¯æ´å¤§è¦æ¨¡æ–‡æª”è™•ç†å’Œé«˜æ•ˆèƒ½æœç´¢ï¼š

```mermaid
erDiagram
    RESOURCES {
        uuid uuid PK
        remote_src_url text UK
        local_src_url text
        content_time timestamptz
        content_header text
        content_authors text
        src_name text
        src_description text
        src_category text
        file_type text
        need_parsed boolean
        crawl_completed boolean
        created_at timestamptz
        updated_at timestamptz
        content_sha256 bytea
        lang text
    }
    
    PARSED_ARTIFACTS {
        uuid uuid PK
        resource_uuid uuid FK
        local_parsed_url text
        parse_setting integer
        created_at timestamptz
        updated_at timestamptz
    }
    
    CHUNKS {
        uuid uuid PK
        resource_uuid uuid FK
        parsed_uuid uuid FK
        image_uuid uuid FK
        page integer
        chunk_order integer
        chunk_setting integer
        token_size integer
        chunking_text text
        description text
        chunk_embedding vector_1536_
        description_embedding vector_1536_
        created_at timestamptz
        updated_at timestamptz
    }
    
    IMAGES {
        uuid uuid PK
        resource_uuid uuid FK
        local_image_url text
        remote_image_url text UK
        description text
        width integer
        height integer
        mime_type text
        created_at timestamptz
        updated_at timestamptz
        image_sha256 bytea
    }
    
    CHUNK_EMBEDDINGS {
        chunk_uuid uuid FK
        kind text
        model text
        dim integer
        embedding vector
        created_at timestamptz
    }
    
    RESOURCES ||--o{ PARSED_ARTIFACTS : "generates"
    RESOURCES ||--o{ IMAGES : "contains"
    RESOURCES ||--o{ CHUNKS : "split_into"
    PARSED_ARTIFACTS ||--o{ CHUNKS : "produces"
    IMAGES ||--o{ CHUNKS : "references"
    CHUNKS ||--o{ CHUNK_EMBEDDINGS : "embeds"
```

### 1. ğŸ“„ **resources** - è³‡æºä¸»è¡¨

å­˜å„²æ‰€æœ‰åŸå§‹æ–‡æª”å’Œè³‡æºï¼š

```sql
CREATE TABLE resources (
    uuid              uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
    remote_src_url    text UNIQUE,                    -- åŸå§‹ URL
    local_src_url     text,                           -- æœ¬åœ°å­˜å„²è·¯å¾‘
    content_time      timestamptz,                    -- å…§å®¹ç™¼å¸ƒæ™‚é–“
    content_header    text,                           -- æ¨™é¡Œ
    content_authors   text,                           -- ä½œè€…
    src_name          text,                           -- ä¾†æºåç¨± (å¦‚ "Bloomberg", "AYZ")
    src_description   text,                           -- ä¾†æºæè¿°
    src_category      text,                           -- åˆ†é¡ (news, paper, blog, docs, web, internal, other)
    file_type         text,                           -- æª”æ¡ˆé¡å‹ (pdf, html, txt, image, audio, video, other)
    need_parsed       boolean NOT NULL DEFAULT false, -- æ˜¯å¦éœ€è¦è§£æ
    crawl_completed   boolean NOT NULL DEFAULT false, -- æ˜¯å¦å·²å®Œæˆçˆ¬å–
    created_at        timestamptz NOT NULL DEFAULT now(),
    updated_at        timestamptz NOT NULL DEFAULT now(),
    content_sha256    bytea,                          -- å…§å®¹é›œæ¹Š (å»é‡)
    lang              text                            -- èªè¨€ä»£ç¢¼ (zh-TW, en, etc.)
);
```

### 2. ğŸ“ **chunks** - æ–‡æœ¬åˆ†å¡Šè¡¨

æ™ºèƒ½åˆ†å¡Šå¾Œçš„æ–‡æœ¬å…§å®¹ï¼Œæ”¯æ´å‘é‡å’Œå…¨æ–‡æœç´¢ï¼š

```sql
CREATE TABLE chunks (
    uuid                    uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
    resource_uuid           uuid NOT NULL REFERENCES resources(uuid),
    parsed_uuid             uuid REFERENCES parsed_artifacts(uuid),
    image_uuid              uuid REFERENCES images(uuid),
    
    page                    integer CHECK (page >= 0),      -- é ç¢¼ (PDF)
    chunk_order             integer NOT NULL CHECK (chunk_order >= 0), -- é †åº
    chunk_setting           integer,                         -- åˆ†å¡Šé…ç½® ID
    token_size              integer CHECK (token_size >= 0), -- Token æ•¸é‡
    
    chunking_text           text NOT NULL,                   -- åˆ†å¡Šæ–‡æœ¬
    description             text,                            -- æ‘˜è¦/æè¿°
    
    -- è‡ªå‹•ç”Ÿæˆçš„å…¨æ–‡æœç´¢å‘é‡
    chunking_text_tsv       tsvector GENERATED ALWAYS AS (to_tsvector('simple', chunking_text)) STORED,
    description_tsv         tsvector GENERATED ALWAYS AS (to_tsvector('simple', description)) STORED,
    
    -- å‘é‡åµŒå…¥ (1536 ç¶­ - OpenAI text-embedding-3-large)
    chunk_embedding         vector(1536),
    description_embedding   vector(1536),
    
    created_at              timestamptz NOT NULL DEFAULT now(),
    updated_at              timestamptz NOT NULL DEFAULT now()
);
```

### 3. ğŸ–¼ **images** - åœ–ç‰‡è³‡æºè¡¨

å­˜å„²èˆ‡æ–‡æª”ç›¸é—œçš„åœ–ç‰‡è³‡æºï¼š

```sql
CREATE TABLE images (
    uuid              uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
    resource_uuid     uuid REFERENCES resources(uuid),
    local_image_url   text,                           -- æœ¬åœ°å­˜å„²è·¯å¾‘
    remote_image_url  text UNIQUE,                    -- åŸå§‹åœ–ç‰‡ URL
    description       text,                           -- åœ–ç‰‡æè¿°
    width             integer,                        -- å¯¬åº¦
    height            integer,                        -- é«˜åº¦
    mime_type         text,                           -- MIME é¡å‹
    created_at        timestamptz NOT NULL DEFAULT now(),
    updated_at        timestamptz NOT NULL DEFAULT now(),
    image_sha256      bytea                           -- åœ–ç‰‡é›œæ¹Š
);
```

### 4. ğŸ”§ **parsed_artifacts** - è§£æç”¢ç‰©è¡¨

å­˜å„²æ–‡æª”è§£æçš„ä¸­é–“çµæœï¼š

```sql
CREATE TABLE parsed_artifacts (
    uuid              uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
    resource_uuid     uuid NOT NULL REFERENCES resources(uuid),
    local_parsed_url  text,                          -- è§£æç”¢ç‰©å­˜å„²è·¯å¾‘
    parse_setting     integer NOT NULL,              -- è§£æé…ç½® ID
    created_at        timestamptz NOT NULL DEFAULT now(),
    updated_at        timestamptz NOT NULL DEFAULT now()
);
```

### 5. ğŸ§® **chunk_embeddings** - å¤šæ¨¡å‹åµŒå…¥è¡¨

æ”¯æ´å¤šç¨®åµŒå…¥æ¨¡å‹çš„å‘é‡å­˜å„²ï¼š

```sql
CREATE TABLE chunk_embeddings (
    chunk_uuid    uuid NOT NULL REFERENCES chunks(uuid),
    kind          text NOT NULL CHECK (kind IN ('chunk','description')),
    model         text NOT NULL,                      -- æ¨¡å‹åç¨± (å¦‚ 'text-embedding-3-large@1536')
    dim           integer NOT NULL,                   -- å‘é‡ç¶­åº¦
    embedding     vector,                            -- å‘é‡æ•¸æ“š
    created_at    timestamptz NOT NULL DEFAULT now(),
    PRIMARY KEY (chunk_uuid, kind, model)
);
```

### ğŸ” æœç´¢åŠŸèƒ½

ç³»çµ±å…§å»ºä¸‰ç¨®å¼·å¤§çš„æœç´¢åŠŸèƒ½ï¼š

#### 1. å‘é‡èªç¾©æœç´¢

```sql
-- ä½¿ç”¨é è¨“ç·´åµŒå…¥æ¨¡å‹é€²è¡Œèªç¾©æœç´¢
SELECT * FROM search_chunks_by_embedding(
    query_embedding := '[0.1, 0.2, 0.3, ...]'::vector(1536),
    match_threshold := 0.8,
    match_count := 10
);
```

#### 2. PostgreSQL å…¨æ–‡æœç´¢

```sql
-- ä½¿ç”¨ PostgreSQL åŸç”Ÿå…¨æ–‡æœç´¢
SELECT * FROM search_chunks_by_text(
    search_query := 'AI äººå·¥æ™ºæ…§ æ©Ÿå™¨å­¸ç¿’',
    match_count := 10
);
```

#### 3. æ··åˆæœç´¢ (æ¨è–¦)

```sql
-- çµåˆå‘é‡æœç´¢å’Œå…¨æ–‡æœç´¢çš„æ··åˆæ¨¡å¼
SELECT * FROM hybrid_search_chunks(
    search_query := 'AI æ©Ÿå™¨å­¸ç¿’',
    query_embedding := '[0.1, 0.2, ...]'::vector(1536),
    text_weight := 0.5,
    vector_weight := 0.5,
    match_count := 10
);
```

### ğŸ“ˆ æ•ˆèƒ½æœ€ä½³åŒ–

ç³»çµ±å·²é è¨­æœ€ä½³åŒ–é…ç½®ï¼š

```sql
-- å‘é‡ç´¢å¼• (IVFFlat - é©åˆå¤§è¦æ¨¡æ•¸æ“š)
CREATE INDEX idx_chunks_chunk_emb_ivf ON chunks 
    USING ivfflat (chunk_embedding vector_cosine_ops) 
    WITH (lists = 100);

-- å…¨æ–‡æœç´¢ç´¢å¼• (GIN)
CREATE INDEX idx_chunks_text_gin ON chunks USING GIN (chunking_text_tsv);

-- æŸ¥è©¢æœ€ä½³åŒ–è¨­å®š
SET ivfflat.probes = 10;  -- å‘é‡æŸ¥è©¢ç²¾åº¦èª¿æ•´
```

## ğŸ Python å®¢æˆ¶ç«¯å®Œæ•´æŒ‡å—

### UV Workspace æ•´åˆ

æœ¬å°ˆæ¡ˆæ¡ç”¨ UV ä½œç‚ºç¾ä»£åŒ–çš„ Python ä¾è³´ç®¡ç†å·¥å…·ï¼Œæä¾›å¿«é€Ÿã€å¯é çš„é–‹ç™¼é«”é©—ï¼š

```bash
# ğŸš€ UV çš„å„ªå‹¢
# - 10-100å€å¿«æ–¼ pip/pipenv
# - çµ±ä¸€çš„ workspace ç®¡ç†
# - ç²¾ç¢ºçš„ä¾è³´è§£æ
# - ç”Ÿç”¢å°±ç·’çš„é–å®šæª”æ¡ˆ

# å®‰è£æ‰€æœ‰ä¾è³´
uv sync

# å®‰è£ç‰¹å®šåŠŸèƒ½çµ„
uv sync --extra ai          # AI/åµŒå…¥åŠŸèƒ½
uv sync --extra monitoring  # ç›£æ§å·¥å…·
uv sync --extra dev         # é–‹ç™¼å·¥å…·

# åŸ·è¡Œç¨‹å¼
uv run python script.py
uv run supabase-cli health
```

### åŸºæœ¬ä½¿ç”¨

#### 1. å¿«é€Ÿé€£æ¥

```python
"""
æœ€ç°¡å–®çš„ä½¿ç”¨æ–¹å¼ - è‡ªå‹•é…ç½®
"""
from client import create_client

# ä½¿ç”¨é è¨­é…ç½® (è‡ªå‹•è¼‰å…¥ .env æª”æ¡ˆ)
client = create_client()

# å¥åº·æª¢æŸ¥
health = client.health_check()
print(f"âœ… é€£æ¥ç‹€æ…‹: {health}")

# Context manager è‡ªå‹•è³‡æºæ¸…ç†
with create_client() as client:
    resources = client.get_all_resources()
    print(f"ğŸ“Š ç¸½è³‡æºæ•¸: {len(resources)}")
```

#### 2. è‡ªå®šç¾©é…ç½®

```python
"""
é€²éšé…ç½® - è‡ªå®šç¾©é€£æ¥åƒæ•¸
"""
from client import create_client, SupabaseConfig

# è‡ªå®šç¾©é…ç½®æª”æ¡ˆ
config = SupabaseConfig(
    env_file='production.env',  # ä½¿ç”¨ç‰¹å®šç’°å¢ƒæª”æ¡ˆ
    database_url='postgresql://user:pass@localhost:5432/db',
    supabase_url='http://localhost:8000',
    supabase_key='your_anon_key'
)

client = create_client(config)

# æˆ–ç›´æ¥å‚³å…¥åƒæ•¸
client = create_client(
    database_url='postgresql://user:pass@localhost:5432/custom_db',
    supabase_url='http://custom-host:8000'
)
```

### è³‡æ–™åº«æ“ä½œ

#### 1. è³‡æºç®¡ç† (Resources)

```python
"""
è³‡æº (Resources) å®Œæ•´æ“ä½œç¯„ä¾‹
"""
from datetime import datetime
from client import create_client

client = create_client()

# ğŸ“ å‰µå»ºæ–°è³‡æº
resource = client.create_resource(
    remote_src_url='https://example.com/article/ai-breakthrough',
    content_header='AI æŠ€è¡“çš„é‡å¤§çªç ´',
    content_authors='å¼µä¸‰, æå››',
    src_name='TechNews',
    src_description='ç§‘æŠ€æ–°èåª’é«”',
    src_category='news',           # å¯å½ˆæ€§ä½¿ç”¨ä»»ä½•å­—ç¬¦ä¸²
    file_type='html',              # æ”¯æ´: html, pdf, txt, image, audio, video, other
    content_time=datetime.now(),
    need_parsed=True,
    lang='zh-TW'
)
print(f"âœ… å‰µå»ºè³‡æº: {resource.uuid}")

# ğŸ” æŸ¥è©¢è³‡æº
# ä¾ UUID æŸ¥è©¢
resource = client.get_resource(resource.uuid)

# ä¾ URL æŸ¥è©¢ (é¿å…é‡è¤‡)
existing = client.get_resources_by_url('https://example.com/article/ai-breakthrough')

# ä¾åˆ†é¡æŸ¥è©¢
news_resources = client.get_resources_by_category('news', limit=50)

# å–å¾—æ‰€æœ‰è³‡æº (åˆ†é )
all_resources = client.get_all_resources(limit=100, offset=0)

# ğŸ”„ æ›´æ–°è³‡æº
updated_resource = client.update_resource(
    resource.uuid,
    content_header='AI æŠ€è¡“çš„é©šäººçªç ´ (æ›´æ–°ç‰ˆ)',
    crawl_completed=True,
    file_type='pdf'  # æ›´æ–°æª”æ¡ˆé¡å‹
)

# ğŸ—‘ åˆªé™¤è³‡æº (ç´šè¯åˆªé™¤ç›¸é—œæ•¸æ“š)
# client.delete_resource(resource.uuid)  # è¬¹æ…ä½¿ç”¨
```

#### 2. æ–‡æœ¬åˆ†å¡Š (Chunks)

```python
"""
æ–‡æœ¬åˆ†å¡Š (Chunks) å®Œæ•´æ“ä½œç¯„ä¾‹
"""
import numpy as np

# ğŸ“ å‰µå»ºæ–‡æœ¬åˆ†å¡Š
chunk = client.create_chunk(
    resource_uuid=resource.uuid,
    chunk_order=1,                          # åœ¨æ–‡æª”ä¸­çš„é †åº
    chunking_text='''
    äººå·¥æ™ºæ…§ (AI) æŠ€è¡“åœ¨è¿‘å¹´ä¾†ç™¼å±•è¿…é€Ÿï¼Œç‰¹åˆ¥æ˜¯åœ¨è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸã€‚
    å¤§å‹èªè¨€æ¨¡å‹ (LLM) å¦‚ ChatGPTã€Claude ç­‰ï¼Œå·²ç¶“å±•ç¾å‡ºé©šäººçš„èƒ½åŠ›ã€‚
    é€™äº›æ¨¡å‹èƒ½å¤ ç†è§£è¤‡é›œçš„èªè¨€çµæ§‹ï¼Œä¸¦ç”¢ç”Ÿé«˜å“è³ªçš„æ–‡æœ¬å…§å®¹ã€‚
    ''',
    description='AI æŠ€è¡“ç™¼å±•æ¦‚è¿° - é‡é»ä»‹ç´¹è‡ªç„¶èªè¨€è™•ç†å’Œå¤§å‹èªè¨€æ¨¡å‹',
    page=1,                                 # PDF é ç¢¼ (å¯é¸)
    chunk_setting=1,                        # åˆ†å¡Šé…ç½® ID
    token_size=150,                         # Token æ•¸é‡
    # åµŒå…¥å‘é‡ (1536 ç¶­ - OpenAI text-embedding-3-large)
    chunk_embedding=np.random.rand(1536).tolist(),
    description_embedding=np.random.rand(1536).tolist()
)

# ğŸ” æŸ¥è©¢åˆ†å¡Š
# ä¾è³‡æºæŸ¥è©¢æ‰€æœ‰åˆ†å¡Š
resource_chunks = client.get_chunks_by_resource(resource.uuid)

# ä¾é †åºæŸ¥è©¢
ordered_chunks = client.get_chunks_by_resource(resource.uuid, order_by='chunk_order')

# ğŸ”„ æ›´æ–°åˆ†å¡Š
updated_chunk = client.update_chunk(
    chunk.uuid,
    description='æ›´æ–°å¾Œçš„æè¿°',
    chunk_embedding=new_embedding_vector
)
```

#### 3. åœ–ç‰‡è³‡æº (Images)

```python
"""
åœ–ç‰‡è³‡æº (Images) æ“ä½œç¯„ä¾‹
"""
# ğŸ“ å‰µå»ºåœ–ç‰‡è¨˜éŒ„
image = client.create_image(
    resource_uuid=resource.uuid,
    remote_image_url='https://example.com/images/ai-chart.png',
    local_image_url='/storage/images/ai-chart-local.png',
    description='AI æŠ€è¡“ç™¼å±•è¶¨å‹¢åœ–è¡¨',
    width=1920,
    height=1080,
    mime_type='image/png'
)

# ğŸ” æŸ¥è©¢åœ–ç‰‡
resource_images = client.get_images_by_resource(resource.uuid)

# ä¾ URL æŸ¥è©¢
image_by_url = client.get_images_by_url('https://example.com/images/ai-chart.png')
```

### é«˜ç´šæœç´¢åŠŸèƒ½

#### 1. å…¨æ–‡æœç´¢

```python
"""
PostgreSQL åŸç”Ÿå…¨æ–‡æœç´¢
æ”¯æ´ä¸­æ–‡ã€è‹±æ–‡å’Œå¤šèªè¨€æœç´¢
"""
# åŸºæœ¬å…¨æ–‡æœç´¢
results = client.search_chunks_by_text(
    search_query='AI äººå·¥æ™ºæ…§ æ©Ÿå™¨å­¸ç¿’',
    limit=20
)

for result in results:
    print(f"ğŸ“„ {result['chunk_uuid']}")
    print(f"ğŸ“ å…§å®¹: {result['chunking_text'][:100]}...")
    print(f"ğŸ¯ ç›¸é—œåº¦: {result['rank']:.3f}")
    print("---")

# ä½¿ç”¨ PostgreSQL å…¨æ–‡æœç´¢æ“ä½œç¬¦
advanced_results = client.search_chunks_by_text(
    # & = AND, | = OR, ! = NOT, <-> = ç›¸é„°
    search_query='(AI | äººå·¥æ™ºæ…§) & (æ©Ÿå™¨å­¸ç¿’ | æ·±åº¦å­¸ç¿’)',
    limit=10
)

# æœç´¢ç‰¹å®šè³‡æºçš„åˆ†å¡Š
resource_search = client.search_chunks_by_text(
    search_query='è‡ªç„¶èªè¨€è™•ç†',
    resource_uuid=resource.uuid,  # é™å®šæœç´¢ç¯„åœ
    limit=10
)
```

#### 2. å‘é‡èªç¾©æœç´¢

```python
"""
ä½¿ç”¨åµŒå…¥å‘é‡é€²è¡Œèªç¾©æœç´¢
é©åˆèªç¾©ç›¸ä¼¼æ€§æŸ¥è©¢
"""
import openai  # ç¯„ä¾‹ä½¿ç”¨ OpenAI

# ç”ŸæˆæŸ¥è©¢åµŒå…¥å‘é‡
query_text = "æ·±åº¦å­¸ç¿’åœ¨è‡ªç„¶èªè¨€è™•ç†ä¸­çš„æ‡‰ç”¨"
query_embedding = openai.Embedding.create(
    model="text-embedding-3-large",
    input=query_text
)['data'][0]['embedding']

# å‘é‡æœç´¢
vector_results = client.search_chunks_by_embedding(
    embedding=query_embedding,
    threshold=0.75,        # ç›¸ä¼¼åº¦é–¾å€¼ (0-1)
    limit=15
)

for result in vector_results:
    print(f"ğŸ§  èªç¾©ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
    print(f"ğŸ“ å…§å®¹: {result['chunking_text'][:150]}...")
    print("---")

# ä½¿ç”¨æè¿°åµŒå…¥æœç´¢ (é€šå¸¸æ›´ç²¾ç¢º)
description_results = client.search_chunks_by_description_embedding(
    embedding=query_embedding,
    threshold=0.8,
    limit=10
)
```

#### 3. æ··åˆæœç´¢ (æ¨è–¦)

```python
"""
æ··åˆæœç´¢çµåˆå…¨æ–‡æœç´¢å’Œå‘é‡æœç´¢çš„å„ªé»
æä¾›æœ€ä½³çš„æœç´¢é«”é©—
"""
# æ··åˆæœç´¢ - å¹³è¡¡æ¬Šé‡
hybrid_results = client.hybrid_search_chunks(
    text_query='AI æ©Ÿå™¨å­¸ç¿’ æ·±åº¦å­¸ç¿’',
    embedding=query_embedding,
    text_weight=0.4,      # å…¨æ–‡æœç´¢æ¬Šé‡
    vector_weight=0.6,    # å‘é‡æœç´¢æ¬Šé‡
    limit=20
)

# åå‘é—œéµå­—æœç´¢çš„æ··åˆæœç´¢
keyword_focused = client.hybrid_search_chunks(
    text_query='ç‰¹å®šæŠ€è¡“åè© API æ¥å£',
    embedding=query_embedding,
    text_weight=0.8,      # æ›´é«˜çš„æ–‡æœ¬æ¬Šé‡
    vector_weight=0.2,
    limit=15
)

# åå‘èªç¾©æœç´¢çš„æ··åˆæœç´¢
semantic_focused = client.hybrid_search_chunks(
    text_query='ç›¸é—œæ¦‚å¿µ',
    embedding=query_embedding,
    text_weight=0.2,
    vector_weight=0.8,    # æ›´é«˜çš„å‘é‡æ¬Šé‡
    limit=15
)

# è™•ç†æœç´¢çµæœ
for result in hybrid_results:
    print(f"ğŸ¯ ç¶œåˆåˆ†æ•¸: {result['combined_score']:.3f}")
    print(f"ğŸ“– è³‡æº: {result['resource_uuid']}")
    print(f"ğŸ“ å…§å®¹: {result['chunking_text'][:200]}...")
    if result['description']:
        print(f"ğŸ“‹ æè¿°: {result['description']}")
    print("=" * 50)
```

### èªè­‰èˆ‡æˆæ¬Š

```python
"""
ç”¨æˆ¶èªè­‰å’Œæ¬Šé™ç®¡ç†
"""
# ç”¨æˆ¶è¨»å†Š
signup_result = client.sign_up(
    email='user@example.com',
    password='SecurePassword123!'
)
print(f"âœ… è¨»å†ŠæˆåŠŸ: {signup_result.user.email}")

# ç”¨æˆ¶ç™»å…¥
signin_result = client.sign_in(
    email='user@example.com', 
    password='SecurePassword123!'
)
print(f"ğŸ”‘ ç™»å…¥æˆåŠŸ: {signin_result.session.access_token}")

# å–å¾—ç•¶å‰ç”¨æˆ¶
current_user = client.get_user()
if current_user:
    print(f"ğŸ‘¤ ç•¶å‰ç”¨æˆ¶: {current_user.email}")

# ç™»å‡º
client.sign_out()
print("ğŸ‘‹ å·²ç™»å‡º")

# é‡è¨­å¯†ç¢¼
client.reset_password('user@example.com')
```

### æª”æ¡ˆå­˜å„²

```python
"""
Supabase Storage - S3 ç›¸å®¹çš„å°è±¡å­˜å„²
"""
# å‰µå»ºå­˜å„²æ¡¶
bucket = client.create_bucket('documents')

# ä¸Šå‚³æª”æ¡ˆ
with open('document.pdf', 'rb') as file:
    upload_result = client.upload_file(
        bucket='documents',
        path='papers/ai-research.pdf',
        file_data=file.read(),
        content_type='application/pdf'
    )

# ä¸‹è¼‰æª”æ¡ˆ
file_data = client.download_file('documents', 'papers/ai-research.pdf')
with open('downloaded.pdf', 'wb') as file:
    file.write(file_data)

# å–å¾—æª”æ¡ˆå…¬é–‹ URL
public_url = client.get_file_url('documents', 'papers/ai-research.pdf')
print(f"ğŸ”— å…¬é–‹é€£çµ: {public_url}")

# åˆªé™¤æª”æ¡ˆ
client.delete_file('documents', 'papers/ai-research.pdf')
```

### å³æ™‚é€šè¨Š (Realtime)

```python
"""
WebSocket å³æ™‚æ•¸æ“šåŒæ­¥
"""
def handle_resource_changes(payload):
    """è™•ç†è³‡æºè¡¨è®Šæ›´äº‹ä»¶"""
    event_type = payload['eventType']  # INSERT, UPDATE, DELETE
    record = payload['new'] if payload['new'] else payload['old']
    
    if event_type == 'INSERT':
        print(f"ğŸ“„ æ–°å¢è³‡æº: {record['content_header']}")
    elif event_type == 'UPDATE':
        print(f"ğŸ”„ æ›´æ–°è³‡æº: {record['content_header']}")
    elif event_type == 'DELETE':
        print(f"ğŸ—‘ åˆªé™¤è³‡æº: {record['uuid']}")

def handle_chunk_changes(payload):
    """è™•ç†æ–‡æœ¬åˆ†å¡Šè®Šæ›´äº‹ä»¶"""
    print(f"ğŸ“ æ–‡æœ¬åˆ†å¡Šè®Šæ›´: {payload['eventType']}")

# è¨‚é–±è³‡æºè¡¨è®Šæ›´
resource_channel = client.subscribe_to_changes(
    table='resources',
    callback=handle_resource_changes
)

# è¨‚é–±æ–‡æœ¬åˆ†å¡Šè®Šæ›´
chunk_channel = client.subscribe_to_changes(
    table='chunks', 
    callback=handle_chunk_changes,
    filter_column='resource_uuid',     # å¯é¸ï¼šéæ¿¾ç‰¹å®šè³‡æº
    filter_value=resource.uuid
)

# ä¿æŒé€£æ¥
print("ğŸ”— å³æ™‚ç›£è½å·²å•Ÿå‹•ï¼ŒæŒ‰ Ctrl+C åœæ­¢...")
try:
    import time
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    # å–æ¶ˆè¨‚é–±
    resource_channel.unsubscribe()
    chunk_channel.unsubscribe()
    print("ğŸ‘‹ å·²åœæ­¢ç›£è½")
```

## ğŸ–¥ å‘½ä»¤åˆ—å·¥å…· (CLI)

æœ¬ç³»çµ±æä¾›äº†åŠŸèƒ½å¼·å¤§çš„ CLI å·¥å…·ï¼Œæ–¹ä¾¿ç®¡ç†å’Œæ“ä½œï¼š

### å®‰è£èˆ‡åŸºæœ¬ä½¿ç”¨

```bash
# CLI å·¥å…·å·²éš¨å¥—ä»¶å®‰è£ï¼Œå¯ç›´æ¥ä½¿ç”¨
uv run supabase-cli --help

# æˆ–åœ¨ UV shell ä¸­ä½¿ç”¨
uv shell
supabase-cli --help
```

### ç³»çµ±ç®¡ç†

```bash
# ğŸ” å¥åº·æª¢æŸ¥
uv run supabase-cli health
# è¼¸å‡º: âœ… ç³»çµ±å¥åº· - PostgreSQL: OK, Supabase: OK

# ğŸ“Š ç³»çµ±çµ±è¨ˆ
uv run supabase-cli stats
# é¡¯ç¤ºï¼šè³‡æºæ•¸ã€åˆ†å¡Šæ•¸ã€åœ–ç‰‡æ•¸ç­‰çµ±è¨ˆè³‡è¨Š

# ğŸ”§ è³‡æ–™åº«é€£æ¥æ¸¬è©¦
uv run supabase-cli db test-connection
```

### è³‡æºç®¡ç†

```bash
# ğŸ“‹ åˆ—å‡ºè³‡æº
uv run supabase-cli resource list --limit 20
uv run supabase-cli resource list --category news
uv run supabase-cli resource list --format json

# â• å‰µå»ºè³‡æº
uv run supabase-cli resource create \
    --url "https://example.com/article" \
    --title "AI æŠ€è¡“æ–°çªç ´" \
    --source "TechNews" \
    --category "news" \
    --file-type "html"

# ğŸ” æŸ¥è©¢ç‰¹å®šè³‡æº
uv run supabase-cli resource get <uuid>
uv run supabase-cli resource get --url "https://example.com/article"

# ğŸ”„ æ›´æ–°è³‡æº
uv run supabase-cli resource update <uuid> \
    --title "æ›´æ–°å¾Œçš„æ¨™é¡Œ" \
    --completed true

# ğŸ—‘ åˆªé™¤è³‡æº (è¬¹æ…ä½¿ç”¨)
uv run supabase-cli resource delete <uuid> --confirm
```

### æœç´¢åŠŸèƒ½

```bash
# ğŸ” å…¨æ–‡æœç´¢
uv run supabase-cli search text "AI æ©Ÿå™¨å­¸ç¿’" --limit 10

# ğŸ§  å‘é‡æœç´¢ (éœ€è¦å…ˆæº–å‚™åµŒå…¥å‘é‡)
uv run supabase-cli search vector \
    --embedding-file embeddings.json \
    --threshold 0.8 \
    --limit 15

# ğŸ¯ æ··åˆæœç´¢
uv run supabase-cli search hybrid \
    --query "æ·±åº¦å­¸ç¿’" \
    --embedding-file embeddings.json \
    --text-weight 0.4 \
    --vector-weight 0.6
```

### æ•¸æ“šç®¡ç†

```bash
# ğŸ“¥ å°å…¥æ•¸æ“š
uv run supabase-cli data import --file resources.json
uv run supabase-cli data import --csv resources.csv --table resources

# ğŸ“¤ å°å‡ºæ•¸æ“š
uv run supabase-cli data export --table resources --format json
uv run supabase-cli data export --table chunks --limit 1000

# ğŸ”„ è³‡æ–™é·ç§»
uv run supabase-cli migration run --file sql/custom_migration.sql
```

## ğŸ”§ ç³»çµ±ç®¡ç†è…³æœ¬

`supabase.sh` æ˜¯ç³»çµ±çš„æ ¸å¿ƒç®¡ç†è…³æœ¬ï¼Œæä¾›å®Œæ•´çš„ç”Ÿå‘½é€±æœŸç®¡ç†ï¼š

### åŸºæœ¬æ“ä½œ

```bash
# ğŸš€ é¦–æ¬¡è¨­ç½® (åŒ…å«ä¾è³´å®‰è£)
./supabase.sh setup

# â–¶ï¸ å•Ÿå‹•æ‰€æœ‰æœå‹™
./supabase.sh start

# â¹ åœæ­¢æ‰€æœ‰æœå‹™
./supabase.sh stop

# ğŸ”„ é‡å•Ÿæœå‹™
./supabase.sh restart

# ğŸ“Š æŸ¥çœ‹æœå‹™ç‹€æ…‹
./supabase.sh status

# ğŸ“‹ æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤
./supabase.sh help
```

### é–‹ç™¼å·¥å…·

```bash
# ğŸ é€²å…¥ Python é–‹ç™¼ç’°å¢ƒ
./supabase.sh python

# ğŸ“¦ å®‰è£/æ›´æ–°ä¾è³´
./supabase.sh install

# ğŸ§ª åŸ·è¡Œæ¸¬è©¦
./supabase.sh test

# ğŸ“ ç¨‹å¼ç¢¼å“è³ªæª¢æŸ¥
./supabase.sh lint

# ğŸ¨ ç¨‹å¼ç¢¼æ ¼å¼åŒ–
./supabase.sh format

# ğŸ“Š å‹åˆ¥æª¢æŸ¥
./supabase.sh typecheck
```

### æ•¸æ“šç®¡ç†

```bash
# ğŸ’¾ å‚™ä»½è³‡æ–™åº«
./supabase.sh backup
# å‰µå»º: backup_20241220_143022.sql.gz

# ğŸ”„ å¾å‚™ä»½é‚„åŸ
./supabase.sh restore backup_20241220_143022.sql.gz

# ğŸ—‘ æ¸…ç†æ‰€æœ‰è³‡æ–™ (è¬¹æ…ä½¿ç”¨)
./supabase.sh clean --confirm

# ğŸ§¹ æ¸…ç† Docker è³‡æº
./supabase.sh cleanup
```

### ç›£æ§èˆ‡é™¤éŒ¯

```bash
# ğŸ“‹ æŸ¥çœ‹æ‰€æœ‰æœå‹™æ—¥èªŒ
./supabase.sh logs

# ğŸ“‹ æŸ¥çœ‹ç‰¹å®šæœå‹™æ—¥èªŒ
./supabase.sh logs db        # è³‡æ–™åº«æ—¥èªŒ
./supabase.sh logs auth      # èªè­‰æœå‹™
./supabase.sh logs storage   # å­˜å„²æœå‹™
./supabase.sh logs kong      # API Gateway
./supabase.sh logs studio    # ç®¡ç†ä»‹é¢

# ğŸ” å³æ™‚ç›£æ§æ—¥èªŒ
./supabase.sh logs --follow

# ğŸ’» é€²å…¥ PostgreSQL CLI
./supabase.sh psql

# ğŸ³ é€²å…¥å®¹å™¨ shell
./supabase.sh shell db       # è³‡æ–™åº«å®¹å™¨
./supabase.sh shell kong     # API Gateway å®¹å™¨
```

### ç¶­è­·èˆ‡æ›´æ–°

```bash
# ğŸ”„ æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ (è‡ªå‹•å‚™ä»½)
./supabase.sh update

# ğŸ”§ é‡å»ºæ‰€æœ‰å®¹å™¨
./supabase.sh rebuild

# ğŸ©º ç³»çµ±å¥åº·æª¢æŸ¥
./supabase.sh health

# ğŸ“Š ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³
./supabase.sh resources

# ğŸ” æª¢æŸ¥é…ç½®æª”æ¡ˆ
./supabase.sh validate-config
```

## ğŸ”’ å®‰å…¨èˆ‡æ¬Šé™ç®¡ç†

### Row Level Security (RLS)

ç³»çµ±é è¨­å•Ÿç”¨äº† Row Level Securityï¼Œæä¾›ç²¾ç´°çš„æ¬Šé™æ§åˆ¶ï¼š

```sql
-- æª¢è¦–ç¾æœ‰æ¬Šé™æ”¿ç­–
SELECT schemaname, tablename, policyname, qual, with_check 
FROM pg_policies 
WHERE schemaname = 'public';

-- è‡ªå®šç¾©æ¬Šé™æ”¿ç­–ç¯„ä¾‹
-- å…è¨±ç”¨æˆ¶åªèƒ½æŸ¥çœ‹è‡ªå·±å‰µå»ºçš„è³‡æº
CREATE POLICY "Users can view own resources" ON resources
    FOR SELECT USING (auth.uid() = created_by);

-- å…è¨±ç‰¹å®šè§’è‰²ç®¡ç†æ‰€æœ‰è³‡æº
CREATE POLICY "Admins can manage all resources" ON resources
    FOR ALL USING (auth.jwt() ->> 'role' = 'admin');
```

### API é‡‘é‘°ç®¡ç†

```bash
# ç”Ÿæˆæ–°çš„ JWT é‡‘é‘°
openssl rand -base64 32

# ä½¿ç”¨ Supabase CLI ç”Ÿæˆ API é‡‘é‘°
npx supabase gen keys --project-ref YOUR_PROJECT_ID
```

### ç’°å¢ƒè®Šæ•¸å®‰å…¨æª¢æŸ¥æ¸…å–®

ç¢ºä¿ç”Ÿç”¢ç’°å¢ƒä¸­ä¿®æ”¹ä»¥ä¸‹è¨­å®šï¼š

- [ ] `POSTGRES_PASSWORD` - ä½¿ç”¨å¼·å¯†ç¢¼ (è‡³å°‘ 16 å­—ç¬¦)
- [ ] `JWT_SECRET` - 32 å­—ç¬¦éš¨æ©Ÿå­—ç¬¦ä¸²
- [ ] `ANON_KEY` - ä½¿ç”¨ JWT_SECRET ç”Ÿæˆ
- [ ] `SERVICE_ROLE_KEY` - ä½¿ç”¨ JWT_SECRET ç”Ÿæˆ
- [ ] `DASHBOARD_USERNAME` & `DASHBOARD_PASSWORD` - ç®¡ç†ä»‹é¢èªè­‰
- [ ] éƒµä»¶è¨­å®š (å¦‚æœä½¿ç”¨èªè­‰åŠŸèƒ½)
- [ ] SSL è­‰æ›¸é…ç½® (ç”Ÿç”¢ç’°å¢ƒ)

## ğŸ“Š æ•ˆèƒ½æœ€ä½³åŒ–èˆ‡ç›£æ§

### è³‡æ–™åº«æ•ˆèƒ½èª¿æ ¡

```sql
-- 1. å‘é‡æœç´¢æœ€ä½³åŒ–
-- èª¿æ•´ IVFFlat ç´¢å¼•åƒæ•¸
SET ivfflat.probes = 10;  -- å¢åŠ ç²¾ç¢ºåº¦ (é è¨­: 1)

-- é‡å»ºå‘é‡ç´¢å¼• (æ•¸æ“šé‡å¤§æ™‚åŸ·è¡Œ)
REINDEX INDEX CONCURRENTLY idx_chunks_chunk_emb_ivf;

-- 2. å…¨æ–‡æœç´¢æœ€ä½³åŒ–
-- æ›´æ–°çµ±è¨ˆè³‡è¨Š
ANALYZE chunks;

-- èª¿æ•´ PostgreSQL å…¨æ–‡æœç´¢é…ç½®
SET default_text_search_config = 'simple';  -- æˆ– 'english', 'chinese' ç­‰

-- 3. æŸ¥è©¢æ•ˆèƒ½åˆ†æ
-- åˆ†ææ…¢æŸ¥è©¢
EXPLAIN (ANALYZE, BUFFERS) 
SELECT * FROM search_chunks_by_embedding('[0.1,0.2,...]', 0.8, 10);

-- æª¢è¦–ç´¢å¼•ä½¿ç”¨æƒ…æ³
SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch
FROM pg_stat_user_indexes 
ORDER BY idx_scan DESC;
```

### ç³»çµ±ç›£æ§

```python
"""
ç³»çµ±ç›£æ§å’Œå¥åº·æª¢æŸ¥
"""
from client import create_client

client = create_client()

# å–å¾—ç³»çµ±çµ±è¨ˆè³‡è¨Š
stats = client.get_database_statistics()
print(f"ğŸ“Š ç³»çµ±çµ±è¨ˆ:")
print(f"   è³‡æºæ•¸: {stats['resources_count']:,}")
print(f"   åˆ†å¡Šæ•¸: {stats['chunks_count']:,}")
print(f"   åœ–ç‰‡æ•¸: {stats['images_count']:,}")
print(f"   åˆ†é¡çµ±è¨ˆ: {stats['by_category']}")

# æª¢æŸ¥è³‡æ–™åº«é€£æ¥
health = client.health_check()
print(f"ğŸ” å¥åº·ç‹€æ…‹: {health}")

# ç›£æ§æœç´¢æ•ˆèƒ½
import time
start_time = time.time()
results = client.search_chunks_by_text("æ¸¬è©¦æŸ¥è©¢", limit=100)
search_time = time.time() - start_time
print(f"âš¡ æœç´¢è€—æ™‚: {search_time:.3f} ç§’")
```

### Docker è³‡æºé™åˆ¶

ç·¨è¼¯ `docker-compose.yml` è¨­å®šè³‡æºé™åˆ¶ï¼š

```yaml
services:
  db:
    # è³‡æ–™åº«è¨˜æ†¶é«”é™åˆ¶
    mem_limit: 2g
    memswap_limit: 2g
    
    # CPU é™åˆ¶
    cpus: 2.0
    
    # è¨­å®š PostgreSQL åƒæ•¸
    environment:
      POSTGRES_INITDB_ARGS: "--data-checksums"
    command: |
      postgres 
      -c shared_preload_libraries=vector
      -c max_connections=200
      -c shared_buffers=512MB
      -c effective_cache_size=1536MB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
```

## ğŸ”„ å‚™ä»½èˆ‡ç½é›£å¾©åŸ

### è‡ªå‹•å‚™ä»½ç­–ç•¥

```bash
# è¨­å®š cron å®šæœŸå‚™ä»½
crontab -e

# æ¯æ—¥å‡Œæ™¨ 2 é»å‚™ä»½
0 2 * * * /path/to/supabase/supabase.sh backup

# æ¯é€±æ—¥å‡Œæ™¨ 3 é»æ¸…ç†èˆŠå‚™ä»½ (ä¿ç•™ 30 å¤©)
0 3 * * 0 find /path/to/supabase/backups -name "*.sql.gz" -mtime +30 -delete
```

### ç½é›£å¾©åŸæµç¨‹

```bash
# 1. åœæ­¢æœå‹™
./supabase.sh stop

# 2. æ¸…ç†ç¾æœ‰è³‡æ–™ (è¬¹æ…!)
./supabase.sh clean --confirm

# 3. é‡æ–°è¨­ç½®
./supabase.sh setup

# 4. å¾å‚™ä»½é‚„åŸ
./supabase.sh restore backup_20241220_143022.sql.gz

# 5. é©—è­‰è³‡æ–™å®Œæ•´æ€§
./supabase.sh psql -c "SELECT COUNT(*) FROM resources;"

# 6. é‡å•Ÿæœå‹™
./supabase.sh start
```

### è·¨ç’°å¢ƒé·ç§»

```bash
# æºç’°å¢ƒå‚™ä»½
./supabase.sh backup --full

# ç›®æ¨™ç’°å¢ƒé‚„åŸ
scp backup_20241220_143022.sql.gz target-server:/path/to/supabase/
ssh target-server "cd /path/to/supabase && ./supabase.sh restore backup_20241220_143022.sql.gz"
```

## ğŸ› æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

#### 1. ğŸš« ç„¡æ³•å•Ÿå‹•æœå‹™

**ç—‡ç‹€**: `./supabase.sh start` å¤±æ•—

**è¨ºæ–·æ­¥é©Ÿ**:
```bash
# æª¢æŸ¥ Docker ç‹€æ…‹
docker ps -a
docker compose logs

# æª¢æŸ¥ç«¯å£ä½”ç”¨
netstat -tlnp | grep 8000
netstat -tlnp | grep 5432

# æª¢æŸ¥ç£ç¢Ÿç©ºé–“
df -h
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# åœæ­¢è¡çªæœå‹™
sudo systemctl stop postgresql  # å¦‚æœæœ¬æ©Ÿæœ‰ PostgreSQL

# æ¸…ç† Docker è³‡æº
docker system prune -f

# é‡æ–°å•Ÿå‹•
./supabase.sh restart
```

#### 2. ğŸ” å‘é‡æœç´¢æ•ˆèƒ½å•é¡Œ

**ç—‡ç‹€**: å‘é‡æœç´¢å›æ‡‰ç·©æ…¢

**è¨ºæ–·**:
```sql
-- æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
SELECT indexname FROM pg_indexes WHERE tablename = 'chunks';

-- æª¢æŸ¥æ•¸æ“šé‡
SELECT COUNT(*) FROM chunks WHERE chunk_embedding IS NOT NULL;

-- åˆ†ææŸ¥è©¢è¨ˆåŠƒ
EXPLAIN (ANALYZE, BUFFERS) 
SELECT * FROM chunks 
ORDER BY chunk_embedding <=> '[0.1,0.2,...]' 
LIMIT 10;
```

**è§£æ±ºæ–¹æ¡ˆ**:
```sql
-- å¦‚æœç¼ºå°‘ç´¢å¼•ï¼Œé‡æ–°å‰µå»º
CREATE INDEX CONCURRENTLY idx_chunks_chunk_emb_ivf ON chunks 
    USING ivfflat (chunk_embedding vector_cosine_ops) 
    WITH (lists = 100);

-- èª¿æ•´æŸ¥è©¢åƒæ•¸
SET ivfflat.probes = 10;

-- æ›´æ–°çµ±è¨ˆè³‡è¨Š
ANALYZE chunks;
```

#### 3. ğŸ”‘ èªè­‰å•é¡Œ

**ç—‡ç‹€**: API å‘¼å«è¿”å› 401 Unauthorized

**æª¢æŸ¥é …ç›®**:
```bash
# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
cat .env | grep -E "(JWT_SECRET|ANON_KEY|SERVICE_ROLE_KEY)"

# é©—è­‰ API é‡‘é‘°
curl -H "apikey: YOUR_ANON_KEY" http://localhost:8000/rest/v1/health
```

**è§£æ±ºæ–¹æ¡ˆ**:
1. é‡æ–°ç”Ÿæˆ JWT secret å’Œ API é‡‘é‘°
2. æ›´æ–° `.env` æª”æ¡ˆ
3. é‡å•Ÿæœå‹™

#### 4. ğŸ’¾ è³‡æ–™åº«é€£æ¥å•é¡Œ

**ç—‡ç‹€**: Python å®¢æˆ¶ç«¯ç„¡æ³•é€£æ¥è³‡æ–™åº«

**è¨ºæ–·**:
```python
# æ¸¬è©¦è³‡æ–™åº«é€£æ¥
import psycopg2
try:
    conn = psycopg2.connect(
        host="localhost",
        port=5432,
        database="postgres",
        user="postgres",
        password="YOUR_POSTGRES_PASSWORD"
    )
    print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ é€£æ¥å¤±æ•—: {e}")
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æª¢æŸ¥è³‡æ–™åº«å®¹å™¨ç‹€æ…‹
docker compose logs db

# é‡å•Ÿè³‡æ–™åº«
docker compose restart db

# æª¢æŸ¥è³‡æ–™åº«åˆå§‹åŒ–
./supabase.sh psql -c "\dt"
```

### æ—¥èªŒåˆ†æ

```bash
# æŸ¥çœ‹ç‰¹å®šæ™‚é–“ç¯„åœçš„æ—¥èªŒ
./supabase.sh logs --since="2024-12-20T10:00:00" --until="2024-12-20T11:00:00"

# éæ¿¾éŒ¯èª¤æ—¥èªŒ
./supabase.sh logs | grep -i error

# ç›£æ§å³æ™‚æ—¥èªŒ
./supabase.sh logs --follow | grep -i "error\|warning\|exception"

# æŸ¥çœ‹è³‡æ–™åº«æ…¢æŸ¥è©¢
./supabase.sh psql -c "
SELECT query, calls, total_time, mean_time 
FROM pg_stat_statements 
ORDER BY mean_time DESC 
LIMIT 10;
"
```

## ğŸ”Œ æ•´åˆèˆ‡æ“´å±•

### æ•´åˆåˆ° StatementDog çˆ¬èŸ²ç³»çµ±

```python
"""
åœ¨åª’é«”ä¾†æºçˆ¬èŸ²ä¸­ä½¿ç”¨ Supabase
ç¯„ä¾‹ï¼šBloomberg çˆ¬èŸ²æ•´åˆ
"""
from supabase.client import create_client
from datetime import datetime

class BloombergCrawler:
    def __init__(self):
        self.supabase_client = create_client()
    
    def crawl_article(self, url: str):
        """çˆ¬å–å–®ç¯‡æ–‡ç« ä¸¦å­˜å…¥ Supabase"""
        
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = self.supabase_client.get_resources_by_url(url)
        if existing:
            print(f"ğŸ“„ æ–‡ç« å·²å­˜åœ¨: {url}")
            return existing
        
        # çˆ¬å–æ–‡ç« å…§å®¹
        article_data = self.scrape_article(url)
        
        # å‰µå»ºè³‡æºè¨˜éŒ„
        resource = self.supabase_client.create_resource(
            remote_src_url=url,
            content_header=article_data['title'],
            content_authors=article_data['authors'],
            src_name='Bloomberg',
            src_description='è²¡ç¶“æ–°èåª’é«”',
            src_category='news',
            file_type='html',
            content_time=article_data['published_at'],
            need_parsed=True
        )
        
        # è™•ç†åœ–ç‰‡
        for img_url in article_data['images']:
            self.supabase_client.create_image(
                resource_uuid=resource.uuid,
                remote_image_url=img_url,
                description=f"Bloomberg æ–‡ç« åœ–ç‰‡: {article_data['title']}"
            )
        
        # æ–‡æœ¬åˆ†å¡Šå’ŒåµŒå…¥
        chunks = self.process_text_chunks(
            text=article_data['content'], 
            resource=resource
        )
        
        # æ¨™è¨˜ç‚ºå·²å®Œæˆ
        self.supabase_client.update_resource(
            resource.uuid,
            crawl_completed=True
        )
        
        return resource
    
    def process_text_chunks(self, text: str, resource):
        """è™•ç†æ–‡æœ¬åˆ†å¡Šå’ŒåµŒå…¥å‘é‡"""
        import openai
        
        # æ™ºèƒ½åˆ†å¡Š (ç¯„ä¾‹)
        chunks = self.smart_chunking(text)
        
        created_chunks = []
        for i, chunk_text in enumerate(chunks):
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = openai.Embedding.create(
                model="text-embedding-3-large",
                input=chunk_text
            )['data'][0]['embedding']
            
            # å‰µå»ºåˆ†å¡Šè¨˜éŒ„
            chunk = self.supabase_client.create_chunk(
                resource_uuid=resource.uuid,
                chunk_order=i + 1,
                chunking_text=chunk_text,
                chunk_embedding=embedding,
                token_size=len(chunk_text.split())
            )
            created_chunks.append(chunk)
        
        return created_chunks
```

### API æ•´åˆç¯„ä¾‹

```python
"""
RESTful API æ•´åˆ
"""
import requests

# ä½¿ç”¨ Supabase REST API
def query_resources_api(supabase_url: str, api_key: str):
    """é€é REST API æŸ¥è©¢è³‡æº"""
    
    headers = {
        'apikey': api_key,
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    # æŸ¥è©¢æœ€æ–° 10 ç¯‡æ–°è
    response = requests.get(
        f"{supabase_url}/rest/v1/resources",
        headers=headers,
        params={
            'select': 'uuid,content_header,src_name,content_time',
            'src_category': 'eq.news',
            'order': 'content_time.desc',
            'limit': 10
        }
    )
    
    return response.json()

# GraphQL æŸ¥è©¢
def query_with_graphql(supabase_url: str, api_key: str):
    """ä½¿ç”¨ GraphQL æŸ¥è©¢"""
    
    query = """
    query GetResourcesWithChunks {
        resources(
            filter: { src_category: { eq: "news" } }
            orderBy: { content_time: DESC }
            first: 5
        ) {
            uuid
            content_header
            src_name
            chunks {
                uuid
                chunking_text
            }
        }
    }
    """
    
    response = requests.post(
        f"{supabase_url}/graphql/v1",
        headers={
            'apikey': api_key,
            'Content-Type': 'application/json'
        },
        json={'query': query}
    )
    
    return response.json()
```

### è‡ªå®šç¾©æ“´å±•

```python
"""
æ“´å±•å®¢æˆ¶ç«¯åŠŸèƒ½
"""
from client import SupabaseClient

class CustomRAGClient(SupabaseClient):
    """è‡ªå®šç¾© RAG å®¢æˆ¶ç«¯ - æ“´å±•åŠŸèƒ½"""
    
    def semantic_search_with_context(self, query: str, context_window: int = 3):
        """èªç¾©æœç´¢ä¸¦è¿”å›ä¸Šä¸‹æ–‡åˆ†å¡Š"""
        
        # ç”ŸæˆæŸ¥è©¢åµŒå…¥
        query_embedding = self.generate_embedding(query)
        
        # åŸ·è¡Œå‘é‡æœç´¢
        results = self.search_chunks_by_embedding(
            embedding=query_embedding,
            threshold=0.7,
            limit=20
        )
        
        # ç‚ºæ¯å€‹çµæœæ·»åŠ ä¸Šä¸‹æ–‡åˆ†å¡Š
        enriched_results = []
        for result in results:
            # å–å¾—ç›¸é„°åˆ†å¡Šä½œç‚ºä¸Šä¸‹æ–‡
            context_chunks = self.get_context_chunks(
                resource_uuid=result['resource_uuid'],
                chunk_order=result['chunk_order'],
                window=context_window
            )
            
            result['context'] = context_chunks
            enriched_results.append(result)
        
        return enriched_results
    
    def get_context_chunks(self, resource_uuid: str, chunk_order: int, window: int):
        """å–å¾—ä¸Šä¸‹æ–‡åˆ†å¡Š"""
        
        min_order = max(1, chunk_order - window)
        max_order = chunk_order + window
        
        return self.db.session.query(self.db.Chunk).filter(
            self.db.Chunk.resource_uuid == resource_uuid,
            self.db.Chunk.chunk_order.between(min_order, max_order)
        ).order_by(self.db.Chunk.chunk_order).all()
    
    def generate_summary(self, resource_uuid: str):
        """ä½¿ç”¨ AI ç”Ÿæˆè³‡æºæ‘˜è¦"""
        
        resource = self.get_resource(resource_uuid)
        chunks = self.get_chunks_by_resource(resource_uuid)
        
        # çµ„åˆæ‰€æœ‰åˆ†å¡Šå…§å®¹
        full_text = "\n".join([chunk.chunking_text for chunk in chunks])
        
        # å‘¼å« AI API ç”Ÿæˆæ‘˜è¦ (ç¯„ä¾‹)
        summary = self.call_ai_summarization(full_text)
        
        # æ›´æ–°è³‡æºæè¿°
        self.update_resource(resource_uuid, src_description=summary)
        
        return summary
```

## ğŸ“š é€²éšé…ç½®

### è‡ªå®šç¾© Docker é…ç½®

#### ç”Ÿç”¢ç’°å¢ƒå„ªåŒ–

```yaml
# docker-compose.prod.yml
version: "3.8"

services:
  db:
    image: postgres:15-alpine
    restart: always
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: postgres
      POSTGRES_USER: postgres
    volumes:
      - ./volumes/db:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    # ç”Ÿç”¢ç’°å¢ƒè³‡æºé™åˆ¶
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    # PostgreSQL æ•ˆèƒ½èª¿æ ¡
    command: |
      postgres
      -c shared_preload_libraries=vector
      -c max_connections=100
      -c shared_buffers=1GB
      -c effective_cache_size=3GB
      -c maintenance_work_mem=256MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200

  # åå‘ä»£ç† (ç”Ÿç”¢ç’°å¢ƒ)
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/ssl:ro
    depends_on:
      - kong
```

#### SSL é…ç½®

```nginx
# nginx.conf
server {
    listen 80;
    server_name your-domain.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name your-domain.com;
    
    ssl_certificate /etc/ssl/cert.pem;
    ssl_certificate_key /etc/ssl/key.pem;
    
    location / {
        proxy_pass http://kong:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

### Schema æ“´å±•

#### æ·»åŠ è‡ªå®šç¾©è¡¨æ ¼

```sql
-- sql/02_custom_extensions.sql
-- æ–‡ç« è©•åˆ†è¡¨
CREATE TABLE IF NOT EXISTS article_ratings (
    id SERIAL PRIMARY KEY,
    resource_uuid uuid NOT NULL REFERENCES resources(uuid) ON DELETE CASCADE,
    user_id uuid,
    rating integer CHECK (rating >= 1 AND rating <= 5),
    comment text,
    created_at timestamptz NOT NULL DEFAULT now(),
    
    UNIQUE(resource_uuid, user_id)
);

-- æ¨™ç±¤ç³»çµ±
CREATE TABLE IF NOT EXISTS tags (
    id SERIAL PRIMARY KEY,
    name text UNIQUE NOT NULL,
    description text,
    color text DEFAULT '#blue',
    created_at timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE IF NOT EXISTS resource_tags (
    resource_uuid uuid NOT NULL REFERENCES resources(uuid) ON DELETE CASCADE,
    tag_id integer NOT NULL REFERENCES tags(id) ON DELETE CASCADE,
    created_at timestamptz NOT NULL DEFAULT now(),
    
    PRIMARY KEY (resource_uuid, tag_id)
);

-- æœç´¢æ­·å²
CREATE TABLE IF NOT EXISTS search_history (
    id SERIAL PRIMARY KEY,
    user_id uuid,
    query_text text,
    query_type text CHECK (query_type IN ('text', 'vector', 'hybrid')),
    results_count integer,
    search_time_ms integer,
    created_at timestamptz NOT NULL DEFAULT now()
);

-- å‰µå»ºç´¢å¼•
CREATE INDEX idx_article_ratings_resource ON article_ratings(resource_uuid);
CREATE INDEX idx_resource_tags_resource ON resource_tags(resource_uuid);
CREATE INDEX idx_resource_tags_tag ON resource_tags(tag_id);
CREATE INDEX idx_search_history_user ON search_history(user_id);
CREATE INDEX idx_search_history_created ON search_history(created_at);
```

#### è‡ªå®šç¾©æœç´¢å‡½æ•¸

```sql
-- sql/03_custom_functions.sql
-- å¸¶æ¨™ç±¤éæ¿¾çš„æœç´¢
CREATE OR REPLACE FUNCTION search_chunks_with_tags(
    search_query text,
    tag_names text[] DEFAULT NULL,
    match_count int DEFAULT 10
)
RETURNS TABLE (
    chunk_uuid uuid,
    resource_uuid uuid,
    chunking_text text,
    description text,
    rank float,
    tags text[]
)
LANGUAGE sql STABLE
AS $$
    WITH tagged_resources AS (
        SELECT DISTINCT r.uuid as resource_uuid,
               array_agg(t.name) as tags
        FROM resources r
        LEFT JOIN resource_tags rt ON r.uuid = rt.resource_uuid
        LEFT JOIN tags t ON rt.tag_id = t.id
        WHERE tag_names IS NULL 
           OR t.name = ANY(tag_names)
        GROUP BY r.uuid
    )
    SELECT 
        c.uuid,
        c.resource_uuid,
        c.chunking_text,
        c.description,
        ts_rank(c.chunking_text_tsv, websearch_to_tsquery('simple', search_query)) as rank,
        tr.tags
    FROM chunks c
    INNER JOIN tagged_resources tr ON c.resource_uuid = tr.resource_uuid
    WHERE c.chunking_text_tsv @@ websearch_to_tsquery('simple', search_query)
    ORDER BY rank DESC
    LIMIT match_count;
$$;

-- ç†±é–€æœç´¢çµ±è¨ˆ
CREATE OR REPLACE FUNCTION get_popular_searches(
    days_back int DEFAULT 7,
    limit_count int DEFAULT 10
)
RETURNS TABLE (
    query_text text,
    search_count bigint,
    avg_results integer
)
LANGUAGE sql STABLE
AS $$
    SELECT 
        query_text,
        COUNT(*) as search_count,
        AVG(results_count)::integer as avg_results
    FROM search_history
    WHERE created_at >= now() - interval '1 day' * days_back
    GROUP BY query_text
    ORDER BY search_count DESC
    LIMIT limit_count;
$$;
```

## ğŸ§ª æ¸¬è©¦èˆ‡å“è³ªä¿è­‰

### å–®å…ƒæ¸¬è©¦

```python
"""
tests/test_client.py - å®¢æˆ¶ç«¯æ¸¬è©¦
"""
import pytest
from datetime import datetime
from client import create_client, SupabaseConfig

@pytest.fixture
def client():
    """æ¸¬è©¦å®¢æˆ¶ç«¯ fixture"""
    config = SupabaseConfig(env_file='.env.test')
    return create_client(config)

@pytest.fixture
def sample_resource_data():
    """ç¯„ä¾‹è³‡æºæ•¸æ“š"""
    return {
        'remote_src_url': 'https://test.example.com/article/123',
        'content_header': 'Test Article Title',
        'content_authors': 'Test Author',
        'src_name': 'Test Source',
        'src_description': 'Test Description',
        'src_category': 'test',
        'file_type': 'html',
        'content_time': datetime.now(),
        'lang': 'en'
    }

class TestSupabaseClient:
    """Supabase å®¢æˆ¶ç«¯æ¸¬è©¦å¥—ä»¶"""
    
    def test_health_check(self, client):
        """æ¸¬è©¦å¥åº·æª¢æŸ¥"""
        health = client.health_check()
        assert health is not None
        assert 'database' in health
        assert 'supabase' in health
    
    def test_create_resource(self, client, sample_resource_data):
        """æ¸¬è©¦å‰µå»ºè³‡æº"""
        resource = client.create_resource(**sample_resource_data)
        
        assert resource is not None
        assert resource.uuid is not None
        assert resource.content_header == sample_resource_data['content_header']
        assert resource.src_category == sample_resource_data['src_category']
    
    def test_get_resource(self, client, sample_resource_data):
        """æ¸¬è©¦ç²å–è³‡æº"""
        # å‰µå»ºæ¸¬è©¦è³‡æº
        created_resource = client.create_resource(**sample_resource_data)
        
        # ç²å–è³‡æº
        retrieved_resource = client.get_resource(created_resource.uuid)
        
        assert retrieved_resource is not None
        assert retrieved_resource.uuid == created_resource.uuid
        assert retrieved_resource.content_header == sample_resource_data['content_header']
    
    def test_search_functionality(self, client):
        """æ¸¬è©¦æœç´¢åŠŸèƒ½"""
        # æ¸¬è©¦å…¨æ–‡æœç´¢
        text_results = client.search_chunks_by_text('test query', limit=5)
        assert isinstance(text_results, list)
        
        # æ¸¬è©¦å‘é‡æœç´¢
        dummy_embedding = [0.1] * 1536  # 1536 ç¶­å‘é‡
        vector_results = client.search_chunks_by_embedding(
            embedding=dummy_embedding,
            threshold=0.5,
            limit=5
        )
        assert isinstance(vector_results, list)
    
    def test_error_handling(self, client):
        """æ¸¬è©¦éŒ¯èª¤è™•ç†"""
        # æ¸¬è©¦ä¸å­˜åœ¨çš„è³‡æº
        non_existent = client.get_resource('00000000-0000-0000-0000-000000000000')
        assert non_existent is None
        
        # æ¸¬è©¦ç„¡æ•ˆæ•¸æ“š
        with pytest.raises(ValueError):
            client.create_resource(
                remote_src_url='invalid-url',
                content_header='',  # ç©ºæ¨™é¡Œæ‡‰è©²å¼•ç™¼éŒ¯èª¤
            )

class TestDatabaseOperations:
    """è³‡æ–™åº«æ“ä½œæ¸¬è©¦"""
    
    def test_chunk_operations(self, client, sample_resource_data):
        """æ¸¬è©¦åˆ†å¡Šæ“ä½œ"""
        # å‰µå»ºè³‡æº
        resource = client.create_resource(**sample_resource_data)
        
        # å‰µå»ºåˆ†å¡Š
        chunk_data = {
            'resource_uuid': resource.uuid,
            'chunk_order': 1,
            'chunking_text': 'This is a test chunk content for testing purposes.',
            'description': 'Test chunk description',
            'token_size': 10
        }
        
        chunk = client.create_chunk(**chunk_data)
        
        assert chunk is not None
        assert chunk.resource_uuid == resource.uuid
        assert chunk.chunk_order == 1
        
        # æ¸¬è©¦ç²å–è³‡æºçš„æ‰€æœ‰åˆ†å¡Š
        resource_chunks = client.get_chunks_by_resource(resource.uuid)
        assert len(resource_chunks) >= 1
        assert any(c.uuid == chunk.uuid for c in resource_chunks)
```

### æ•´åˆæ¸¬è©¦

```python
"""
tests/test_integration.py - æ•´åˆæ¸¬è©¦
"""
import pytest
import time
from client import create_client

class TestIntegration:
    """æ•´åˆæ¸¬è©¦å¥—ä»¶"""
    
    def test_full_workflow(self, client):
        """æ¸¬è©¦å®Œæ•´å·¥ä½œæµç¨‹"""
        # 1. å‰µå»ºè³‡æº
        resource = client.create_resource(
            remote_src_url='https://integration-test.com/article',
            content_header='Integration Test Article',
            src_name='Integration Tests',
            src_category='test',
            file_type='html'
        )
        
        # 2. æ·»åŠ åœ–ç‰‡
        image = client.create_image(
            resource_uuid=resource.uuid,
            remote_image_url='https://integration-test.com/image.jpg',
            description='Test image'
        )
        
        # 3. å‰µå»ºå¤šå€‹åˆ†å¡Š
        chunks = []
        for i in range(3):
            chunk = client.create_chunk(
                resource_uuid=resource.uuid,
                chunk_order=i + 1,
                chunking_text=f'Chunk {i+1} content for testing integration workflow.',
                description=f'Chunk {i+1} description'
            )
            chunks.append(chunk)
        
        # 4. æ¸¬è©¦æœç´¢
        search_results = client.search_chunks_by_text('integration', limit=10)
        
        # é©—è­‰çµæœ
        assert len(chunks) == 3
        assert len(search_results) >= 1
        assert any('integration' in result['chunking_text'].lower() 
                  for result in search_results)
    
    def test_performance_benchmarks(self, client):
        """æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
        # æ¸¬è©¦å¤§é‡æ•¸æ“šæ’å…¥
        start_time = time.time()
        
        resources = []
        for i in range(100):
            resource = client.create_resource(
                remote_src_url=f'https://perf-test.com/article/{i}',
                content_header=f'Performance Test Article {i}',
                src_name='Performance Tests',
                src_category='benchmark'
            )
            resources.append(resource)
        
        insert_time = time.time() - start_time
        print(f"æ’å…¥ 100 å€‹è³‡æºè€—æ™‚: {insert_time:.2f} ç§’")
        
        # æ¸¬è©¦æœç´¢æ•ˆèƒ½
        start_time = time.time()
        results = client.search_chunks_by_text('Performance', limit=50)
        search_time = time.time() - start_time
        
        print(f"æœç´¢è€—æ™‚: {search_time:.3f} ç§’")
        print(f"æœç´¢çµæœæ•¸: {len(results)}")
        
        # æ•ˆèƒ½æ–·è¨€
        assert insert_time < 30.0  # æ’å…¥æ‡‰åœ¨ 30 ç§’å…§å®Œæˆ
        assert search_time < 1.0   # æœç´¢æ‡‰åœ¨ 1 ç§’å…§å®Œæˆ
```

### åŸ·è¡Œæ¸¬è©¦

```bash
# è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
cp .env .env.test
# ç·¨è¼¯ .env.testï¼Œä½¿ç”¨æ¸¬è©¦è³‡æ–™åº«

# åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
./supabase.sh test

# åŸ·è¡Œç‰¹å®šæ¸¬è©¦æª”æ¡ˆ
uv run pytest tests/test_client.py -v

# åŸ·è¡Œæ¸¬è©¦ä¸¦ç”Ÿæˆè¦†è“‹ç‡å ±å‘Š
uv run pytest --cov=client --cov-report=html

# æ•ˆèƒ½æ¸¬è©¦
uv run pytest tests/test_integration.py::TestIntegration::test_performance_benchmarks -s
```

## ğŸ“ˆ ç‰ˆæœ¬æ›´æ–°èˆ‡ç¶­è­·

### ç‰ˆæœ¬ç®¡ç†

```bash
# æŸ¥çœ‹ç•¶å‰ç‰ˆæœ¬
./supabase.sh version

# æª¢æŸ¥æ›´æ–°
./supabase.sh check-updates

# æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ (è‡ªå‹•å‚™ä»½)
./supabase.sh update

# å›æ»¾åˆ°ä¹‹å‰ç‰ˆæœ¬
./supabase.sh rollback --version 1.0.0
```

### ç¶­è­·è¨ˆåŠƒ

#### æ¯æ—¥ç¶­è­·ä»»å‹™

```bash
#!/bin/bash
# daily-maintenance.sh

# å¥åº·æª¢æŸ¥
./supabase.sh health

# æ¸…ç†æ—¥èªŒ (ä¿ç•™ 7 å¤©)
find ./volumes/logs -name "*.log" -mtime +7 -delete

# è³‡æ–™åº«çµ±è¨ˆæ›´æ–°
./supabase.sh psql -c "ANALYZE;"

# å‚™ä»½ (å¦‚æœé…ç½®äº†è‡ªå‹•å‚™ä»½)
./supabase.sh backup --quiet
```

#### é€±é–“ç¶­è­·ä»»å‹™

```bash
#!/bin/bash
# weekly-maintenance.sh

# æª¢æŸ¥ç£ç¢Ÿä½¿ç”¨ç‡
df -h ./volumes/

# æ¸…ç†èˆŠå‚™ä»½ (ä¿ç•™ 4 é€±)
find ./backups -name "*.sql.gz" -mtime +28 -delete

# é‡å»ºå‘é‡ç´¢å¼• (å¦‚æœéœ€è¦)
./supabase.sh psql -c "REINDEX INDEX CONCURRENTLY idx_chunks_chunk_emb_ivf;"

# æ›´æ–°ä¾è³´
uv sync --upgrade

# åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶
./supabase.sh test
```

## ğŸ”— ç›¸é—œè³‡æºèˆ‡ç¤¾ç¾¤

### å®˜æ–¹æ–‡æª”

- [Supabase å®˜æ–¹æ–‡æª”](https://supabase.com/docs) - å®Œæ•´çš„ Supabase æŒ‡å—
- [pgvector GitHub](https://github.com/pgvector/pgvector) - PostgreSQL å‘é‡æ“´å±•
- [PostgreSQL å®˜æ–¹æ–‡æª”](https://www.postgresql.org/docs/) - PostgreSQL æ•¸æ“šåº«æ–‡æª”
- [UV åŒ…ç®¡ç†å™¨](https://github.com/astral-sh/uv) - ç¾ä»£ Python åŒ…ç®¡ç†

### ç›¸é—œæŠ€è¡“

- [LangChain](https://langchain.com/) - LLM æ‡‰ç”¨ç¨‹å¼æ¡†æ¶
- [Embeddings æ¨¡å‹æ¯”è¼ƒ](https://huggingface.co/spaces/mteb/leaderboard) - å‘é‡åµŒå…¥æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ
- [Docker Compose](https://docs.docker.com/compose/) - å®¹å™¨ç·¨æ’æ–‡æª”

### ç¤¾ç¾¤èˆ‡æ”¯æ´

- [StatementDog GitHub](https://github.com/your-org/statementdog) - å°ˆæ¡ˆ GitHub å€‰åº«
- [Supabase Discord](https://discord.supabase.com/) - Supabase å®˜æ–¹ç¤¾ç¾¤
- [PostgreSQL ç¤¾ç¾¤](https://www.postgresql.org/community/) - PostgreSQL å®˜æ–¹ç¤¾ç¾¤

## â“ å¸¸è¦‹å•é¡Œ (FAQ)

### Q: å¦‚ä½•æ›´æ›åµŒå…¥æ¨¡å‹ï¼Ÿ

**A**: ä¿®æ”¹ Python ä»£ç¢¼ä¸­çš„åµŒå…¥ç”Ÿæˆå‡½æ•¸ï¼Œä¸¦ç›¸æ‡‰èª¿æ•´å‘é‡ç¶­åº¦ï¼š

```python
# ç¯„ä¾‹ï¼šå¾ OpenAI åˆ‡æ›åˆ° Sentence Transformers
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')  # 384 ç¶­
embedding = model.encode(text).tolist()

# ç›¸æ‡‰èª¿æ•´è³‡æ–™åº« schema
# ALTER TABLE chunks ALTER COLUMN chunk_embedding TYPE vector(384);
```

### Q: å¦‚ä½•è™•ç†å¤§é‡æ•¸æ“šå°å…¥ï¼Ÿ

**A**: ä½¿ç”¨æ‰¹é‡å°å…¥ç­–ç•¥ï¼š

```python
def batch_import_resources(resources_data, batch_size=100):
    """æ‰¹é‡å°å…¥è³‡æº"""
    for i in range(0, len(resources_data), batch_size):
        batch = resources_data[i:i+batch_size]
        
        # ä½¿ç”¨äº‹å‹™è™•ç†
        with client.db.session.begin():
            for resource_data in batch:
                client.create_resource(**resource_data)
        
        print(f"å·²è™•ç† {min(i+batch_size, len(resources_data))}/{len(resources_data)} å€‹è³‡æº")
```

### Q: å¦‚ä½•å„ªåŒ–å‘é‡æœç´¢æ€§èƒ½ï¼Ÿ

**A**: è€ƒæ…®ä»¥ä¸‹å„ªåŒ–ç­–ç•¥ï¼š

1. **èª¿æ•´ç´¢å¼•åƒæ•¸**ï¼šå¢åŠ  `ivfflat.probes` å€¼æé«˜ç²¾ç¢ºåº¦
2. **ä½¿ç”¨ HNSW ç´¢å¼•**ï¼šå°æ–¼é«˜ç¶­æ•¸æ“šï¼ŒHNSW å¯èƒ½æ›´å¿«
3. **é è¨ˆç®—å¸¸ç”¨æŸ¥è©¢**ï¼šå°‡ç†±é–€æœç´¢çµæœç·©å­˜
4. **åˆ†å±¤æœç´¢**ï¼šå…ˆç²—ç¯©å†ç²¾æœ

### Q: å¦‚ä½•å¯¦ç¾å¤šç§Ÿæˆ¶æ¶æ§‹ï¼Ÿ

**A**: ä½¿ç”¨ Row Level Security (RLS) å¯¦ç¾ï¼š

```sql
-- æ·»åŠ ç§Ÿæˆ¶æ¬„ä½
ALTER TABLE resources ADD COLUMN tenant_id uuid;

-- å‰µå»ºç§Ÿæˆ¶éš”é›¢æ”¿ç­–
CREATE POLICY "tenant_isolation" ON resources
    FOR ALL USING (tenant_id = current_setting('app.tenant_id')::uuid);
```

### Q: å¦‚ä½•ç›£æ§ç³»çµ±æ€§èƒ½ï¼Ÿ

**A**: ä½¿ç”¨å…§å»ºçš„ç›£æ§å·¥å…·ï¼š

```python
# ç³»çµ±ç›£æ§è…³æœ¬
def monitor_system():
    client = create_client()
    
    # æª¢æŸ¥è³‡æ–™åº«é€£æ¥
    health = client.health_check()
    
    # æª¢æŸ¥æœç´¢æ€§èƒ½
    start = time.time()
    client.search_chunks_by_text("test", limit=1)
    search_time = time.time() - start
    
    # æª¢æŸ¥ç£ç¢Ÿä½¿ç”¨ç‡
    disk_usage = shutil.disk_usage('./volumes/')
    
    return {
        'health': health,
        'search_time': search_time,
        'disk_free_gb': disk_usage.free / (1024**3)
    }
```

---

## ğŸš€ é–‹å§‹ä½¿ç”¨

æº–å‚™å¥½äº†å—ï¼Ÿè®“æˆ‘å€‘é–‹å§‹ï¼š

```bash
# 1. é€²å…¥å°ˆæ¡ˆç›®éŒ„
cd statementdog/supabase

# 2. é¦–æ¬¡è¨­ç½®
./supabase.sh setup

# 3. é…ç½®ç’°å¢ƒè®Šæ•¸
nano .env  # ä¿®æ”¹å¯†ç¢¼å’Œé‡‘é‘°

# 4. å•Ÿå‹•æœå‹™
./supabase.sh start

# 5. é©—è­‰éƒ¨ç½²
uv run python -c "from client import create_client; print('âœ… æˆåŠŸ:', create_client().health_check())"

# 6. é–‹å§‹é–‹ç™¼ï¼
uv run supabase/uv-example.py
```

ğŸ‰ **æ­å–œï¼æ‚¨çš„ Supabase RAG ç³»çµ±å·²å°±ç·’ï¼**

è¨ªå• [http://localhost:8000](http://localhost:8000) é–‹å§‹ä½¿ç”¨ Supabase Studio ç®¡ç†æ‚¨çš„æ•¸æ“šã€‚

---

**éœ€è¦å”åŠ©ï¼Ÿ** æŸ¥çœ‹æ•…éšœæ’é™¤ç« ç¯€æˆ–æäº¤ [GitHub Issue](https://github.com/your-org/statementdog/issues)ã€‚

**Â© 2024 StatementDog RAG System. All rights reserved.**